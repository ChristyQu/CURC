# -*- coding: utf-8 -*-
"""CURC project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mXGkJ3CnJDFqPZ9Ml6aRRZACk9kSsR-0
"""

# https://www.kaggle.com/tanmoyx/covid19-patient-precondition-dataset?select=covid.csv
# sex: Female - 1, Male - 2
# patien_type:Outpatient - 1, Inpatient - 2
# intubed:Yes - 1, No - 2, Data missing or NA - 97,98,99
# pneumonia:Yes - 1, No - 2, Data missing or NA - 97,98,99
# age: continues variable
# pregnancy Yes - 1, No - 2, Data missing or NA - 97,98,99
# diabetes Yes - 1, No - 2, Data missing or NA - 97,98,99
# copd Yes - 1, No - 2, Data missing or NA - 97,98,99
# asthma Yes - 1, No - 2, Data missing or NA - 97,98,99
# inmsupr Yes - 1, No - 2, Data missing or NA - 97,98,99
# hypertension Yes - 1, No - 2, Data missing or NA - 97,98,99
# other_disease Yes - 1, No - 2, Data missing or NA - 97,98,99
# cardiovascular Yes - 1, No - 2, Data missing or NA - 97,98,99
# obesity Yes - 1, No - 2, Data missing or NA - 97,98,99
# renal_chronic Yes - 1, No - 2, Data missing or NA - 97,98,99
# tobacco Yes - 1, No - 2, Data missing or NA - 97,98,99
# contact_other_covid Yes - 1, No - 2, Data missing or NA - 97,98,99
# covid_res Positive - 1, Negative - 2, Awaiting Results - 3
# icu Yes - 1, No - 2, Data missing or NA - 97,98,99

import pandas as pd
import datetime
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets

# read data
df = pd.read_csv('covid.csv')

df.head()

df.columns

df.describe()

# missing values
# avoid acciendentally delete ages = 97,98,99

#intubed
df=df[df['intubed']!=99]
df=df[df['intubed']!=98]
df=df[df['intubed']!=97]

#pneumonia
df=df[df['pneumonia']!=99]
df=df[df['pneumonia']!=98]
df=df[df['pneumonia']!=97]

#pregnancy
df=df[df['pregnancy']!=99]
df=df[df['pregnancy']!=98]
df=df[df['pregnancy']!=97]

#diabetes
df=df[df['diabetes']!=99]
df=df[df['diabetes']!=98]
df=df[df['diabetes']!=97]

#copd
df=df[df['copd']!=99]
df=df[df['copd']!=98]
df=df[df['copd']!=97]

#asthma
df=df[df['asthma']!=99]
df=df[df['asthma']!=98]
df=df[df['asthma']!=97]

#inmsupr
df=df[df['inmsupr']!=99]
df=df[df['inmsupr']!=98]
df=df[df['inmsupr']!=97]

#hypertension
df=df[df['hypertension']!=99]
df=df[df['hypertension']!=98]
df=df[df['hypertension']!=97]

#other_disease
df=df[df['other_disease']!=99]
df=df[df['other_disease']!=98]
df=df[df['other_disease']!=97]	

#cardiovascular
df=df[df['cardiovascular']!=99]
df=df[df['cardiovascular']!=98]
df=df[df['cardiovascular']!=97]

#obesity
df=df[df['obesity']!=99]
df=df[df['obesity']!=98]
df=df[df['obesity']!=97]

#renal_chronic	
df=df[df['renal_chronic']!=99]
df=df[df['renal_chronic']!=98]
df=df[df['renal_chronic']!=97]

#tobacco
df=df[df['tobacco']!=99]
df=df[df['tobacco']!=98]
df=df[df['tobacco']!=97]

#contact_other_covid
df=df[df['contact_other_covid']!=99]
df=df[df['contact_other_covid']!=98]
df=df[df['contact_other_covid']!=97]

#covid_res
df=df[df['covid_res']!=99]
df=df[df['covid_res']!=98]
df=df[df['covid_res']!=97]

#icu
df=df[df['icu']!=99]
df=df[df['icu']!=98]
df=df[df['icu']!=97]

df.head()
df.describe()

# all sex = 1 and all patient_type = 2, drop the first two columns: only focus on female inpatient
# we don't care about id: drop it

df = df.drop(['id','sex','patient_type'], axis = 1) 

df.describe()
df.head()

# New column fatality: Yes-dead; No-Recovered
# df['fatality'] = np.where(df['date_died'] != '9999-99-99', 'Yes', 'No')
df['fatality'] = np.where(df['date_died'] != '9999-99-99', 1, 2)

# calculate entry - symptoms; only include valid records
df['entry_symptoms'] = pd.DataFrame(pd.to_datetime(df['entry_date']) - pd.to_datetime(df['date_symptoms']))

# dropping units
df['entry_symptoms'] = pd.to_numeric(df['entry_symptoms'].astype(str).str[:-4], errors='coerce')

# only include valid records
df = df[df['entry_symptoms'] >= 0]

df.head()

# drop useless columns
df = df.drop(['entry_date','date_symptoms','date_died'], axis = 1) 

df.describe()

conditions = [
    (df['entry_symptoms'] <= 1),
    (df['entry_symptoms'] > 1) & (df['entry_symptoms'] <= 3),
    (df['entry_symptoms'] > 3) & (df['entry_symptoms'] <= 5),
    (df['entry_symptoms'] > 5) & (df['entry_symptoms'] <= 10),
    (df['entry_symptoms'] > 10)
    ]

values = ['0', '1', '3', '5','10']

df['date_diff_level'] = np.select(conditions, values)

plt.hist(df['date_diff_level'], edgecolor='k', alpha=0.35)
plt.show()

df.head()

# no longer need entry-symptoms: drop column
df = df.drop(['entry_symptoms'], axis = 1) 

df.head()

df.describe()

df.to_csv(path_or_buf="data_ready.csv")

#CLASSIFICATION TREE

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

dt = DecisionTreeClassifier() # default tree
dt.fit(x_train,y_train) 
dt.predict(x_test)

dt.score(x_test, y_test)

dt1 = DecisionTreeClassifier(random_state = 66)
score = cross_val_score(dt1,x_train,y_train,cv=10).mean()
print('gini score: %.5f'%score)
dt2 = DecisionTreeClassifier(criterion = 'entropy',random_state = 66)
score = cross_val_score(dt2,x_train,y_train,cv=10).mean()
print('entropy score: %.5f'%score)

"""

It can be seen above that the result using entropy is slightly better than using gini.

"""

# draw the plot for parameter:max_depth
ScoreAll = []
for i in range(1,100,10):
    dt = DecisionTreeClassifier(criterion = 'entropy',max_depth = i,random_state = 66)
    score = cross_val_score(dt,x,y,cv=10).mean()
    ScoreAll.append([i,score])
ScoreAll = np.array(ScoreAll)

max_score = np.where(ScoreAll==np.max(ScoreAll[:,1]))[0][0]
print("best parameter and score:",ScoreAll[max_score])  
# print(ScoreAll[,0])
plt.figure(figsize=[20,5])
plt.plot(ScoreAll[:,0],ScoreAll[:,1])
plt.show()

param = {'criterion':['gini'],'max_depth':[15,20,30,50,60,100],'min_samples_leaf':[2,3,5,10],'min_impurity_decrease':[0.1,0.2,0.5,0.7]}
grid = GridSearchCV(DecisionTreeClassifier(),param_grid=param,cv=10)
grid.fit(x_train,y_train)
print('best classifier:',grid.best_params_,'best score:', grid.best_score_)

dt3 = DecisionTreeClassifier(max_depth=15,min_samples_leaf=2,min_impurity_decrease=0.1)
dt3.fit(x_train,y_train)
y_pred = dt3.predict(x_test)
print('train set score', dt3.score(x_train,y_train),'test set score',dt3.score(x_test,y_test))

# RANDOM FOREST

y = df['fatality']
x = df.drop('fatality',axis = 1)

# all default
rf0 = RandomForestClassifier(oob_score=True, random_state=10)
rf0.fit(x,y)
print(rf0.oob_score_)
y_predprob = rf0.predict_proba(x)[:,1]
print("AUC Score (Train): %f" % metrics.roc_auc_score(y, y_predprob))

print(rf0.feature_importances_)

feat_importances = pd.Series(rf0.feature_importances_, index=x.columns)
feat_importances.nlargest(5).plot(kind='barh')

param_test1 = {'n_estimators':range(10,201,10)}
gsearch1 = GridSearchCV(estimator = RandomForestClassifier(min_samples_split=100,
                                  min_samples_leaf=20,max_depth=8,max_features='sqrt',random_state=10), 
                       param_grid = param_test1, scoring='roc_auc',cv=5)
gsearch1.fit(x,y)
print(gsearch1.best_params_, gsearch1.best_score_)

param_test2 = {'max_depth':range(2,18,2)}
gsearch2 = GridSearchCV(estimator = RandomForestClassifier(n_estimators=70,min_samples_split=100, 
                                  min_samples_leaf=20,max_features='sqrt',oob_score=True, random_state=10),
   param_grid = param_test2, scoring='roc_auc',iid=False, cv=5)
gsearch2.fit(x,y)
print(gsearch2.best_params_, gsearch2.best_score_)

param_test3 = {'min_samples_split':range(80,150,20), 'min_samples_leaf':range(5,50,5)}
gsearch3 = GridSearchCV(estimator = RandomForestClassifier(n_estimators=70, max_depth=8,
                                  max_features='sqrt' ,oob_score=True, random_state=10),
   param_grid = param_test3, scoring='roc_auc',iid=False, cv=5)
gsearch3.fit(x,y)
print(gsearch3.best_params_, gsearch3.best_score_)

param_test4 = {'max_features':range(2,18,1)}
gsearch4 = GridSearchCV(estimator = RandomForestClassifier(n_estimators=70, max_depth=8, min_samples_split=80,
                                  min_samples_leaf=10 ,oob_score=True, random_state=10),
   param_grid = param_test4, scoring='roc_auc',iid=False, cv=5)
gsearch4.fit(x,y)
print(gsearch4.best_params_, gsearch4.best_score_)

rf1 = RandomForestClassifier(n_estimators= 70, max_depth=8, min_samples_split=80,
                                  min_samples_leaf=10,max_features=4 ,oob_score=True, random_state=10)
rf1.fit(x,y)
rf1.oob_score_

print(rf1.feature_importances_)

feat_importances = pd.Series(rf0.feature_importances_, index=x.columns)
feat_importances.nlargest(5).plot(kind='barh')

#BOOSTING

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]

for learning_rate in lr_list:
    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)
    gb_clf.fit(x_train, y_train)

    print("Learning rate: ", learning_rate)
    print("Accuracy score (training):", gb_clf.score(x_train, y_train))
    print("Accuracy score (validation)",gb_clf.score(x_test, y_test))

# select learning rate = 0.75

gb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.75, max_features=2, max_depth=2, random_state=0)
gb_clf2.fit(x_train, y_train)
predictions = gb_clf2.predict(x_test)

print("Confusion Matrix:")
print(confusion_matrix(y_test, predictions))

print("Classification Report")
print(classification_report(y_test, predictions))

gb_clf2.feature_importances_

feat_importances = pd.Series(gb_clf2.feature_importances_, index=x.columns)
feat_importances.nlargest(5).plot(kind='barh')

x_train['date_diff_level'] = pd.to_numeric(x_train['date_diff_level'])
x_test['date_diff_level'] = pd.to_numeric(x_test['date_diff_level'])

xgb_clf3 = XGBClassifier()
xgb_clf3.fit(x_train, y_train)

score = xgb_clf3.score(x_test, y_test)
print(score)

xgb_clf3.feature_importances_

feat_importances = pd.Series(xgb_clf3.feature_importances_, index=x.columns)
feat_importances.nlargest(5).plot(kind='barh')

"""END OF CODE

"""